adam optimizer for batch layers
same input / output format for every function,
ie every layer takes a vector<...> *input and outputs a vector<...> * which we set to a tensor object

dropout function for tensor

use function pitners for each layers input and output to avoid
unessecary copes



### Pruning

https://www.youtube.com/watch?v=sZzc6tAtTrM - 40 min in 

use gamma from batch norm to determin which filters/output channels to prune 
ie prune the lowest gamma value channel every n iterations